{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "## Topic : Stereo reconstruction and Non-linear optimization\n",
    "\n",
    "#### Instructions\n",
    "<ul>\n",
    "    <li> The second project of the course is designed to get you familiar with stereo reconstruction, and non-linear optimization </li>\n",
    "    <li> Use python for this project. PILLOW and OpenCV are permitted for image I/O. </li>\n",
    "    <li> Submit this notebook as a zipped file on moodle. The format should be $<$team_id$>$_$<$team_ name$>$.zip. Both members have to submit this zip file. </li>\n",
    "    <li> A seperate report is not needed if you're coding in the notebook itself. Please provide adequate descriptions of the approaches you've taken. Also mention work distribution for the two members. </li>\n",
    "    <li> Refer to the late day policy. Start early </li> \n",
    "    <li> Download data from here: https://iiitaphyd-my.sharepoint.com/:f:/g/personal/aryan_sakaria_students_iiit_ac_in/Er5C7351IAlFsvwHUesFeSQBQtlSiAS7AORSEJT2qH_8_w?e=ol98k9  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### PART 1: Stereo dense reconstruction\n",
    "\n",
    "3-D point clouds are very useful in robotics for several tasks such as object detection, motion estimation (3D-3D matching or 3D-2D matching), SLAM, and other forms of scene understanding.  Stereo camerasprovide  us  with  a  convenient  way  to  generate  dense  point  clouds.Densehere,  in  contrast  tosparse,means all the image points are used for the reconstruction.  In this part of the assignment you will begenerating a dense 3D point cloud reconstruction of a scene from stereo images.\n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Generate a disparity map for each stereo pair.  Use OpenCV (e.g.  StereoSGBM) for this.  Notethat the images provided are already rectified and undistorted. </li>\n",
    "    <li> Then, using the camera parameters and baseline information generate colored point clouds fromeach disparity map.  Some points will have invalid disparity values, so ignore them.  Use [Open3D]for storing your point clouds. </li>\n",
    "    <li> Register (or transform) all the generated point clouds into your world frame by using the providedground truth poses. </li>\n",
    "    <li> Visualize the registered point cloud data, in color.  Use Open3D for this </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries:\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import normalize #normalizing gives better results. Experiment with this\n",
    "import cv2\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_transformations(filename='poses.txt'):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    transformation_list = []\n",
    "    for i in range(len(lines)):\n",
    "        transformation_list_temp = lines[i].split()\n",
    "        temp_rot = [] \n",
    "        temp_rot.append( (transformation_list_temp[0:4] ) ) \n",
    "        temp_rot.append( (transformation_list_temp[4:8]  ) ) \n",
    "        temp_rot.append( (transformation_list_temp[8:12]  ) ) \n",
    "        transformation_list.append(temp_rot)\n",
    "    return transformation_list\n",
    "   \n",
    "\n",
    "def newpoint(dp3, x):\n",
    "    \n",
    "    x1 = np.array([\n",
    "        [x[0][0], x[0][1], x[0][2], x[0][3]],\n",
    "        [x[1][0], x[1][1], x[1][2], x[1][3]],\n",
    "        [x[2][0], x[2][1], x[2][2], x[2][3]],\n",
    "    ])\n",
    "    pts = np.mat([dp3[0], dp3[1], dp3[2], 1]).T\n",
    "    mats = np.matmul(x1, pts)\n",
    "    return mats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide explanation in this cell: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n",
      "[[240. 240. 240. ... 240. 240. 240.]\n",
      " [240. 240. 240. ... 240. 240. 240.]\n",
      " [240. 240. 240. ... 368. 240. 240.]\n",
      " ...\n",
      " [240. 240. 240. ... 240. 240. 240.]\n",
      " [240. 240. 240. ... 240. 240. 240.]\n",
      " [240. 240. 240. ... 240. 240. 240.]]\n"
     ]
    }
   ],
   "source": [
    "# your code here \n",
    "'''\n",
    "The total number of point clouds that we will recieve is \n",
    "There are 40 cameras (or 20 stereo camera)\n",
    "'''\n",
    "\n",
    "# Number of pixels in all 40 images. \n",
    "\n",
    "calibF=open('calib.txt')\n",
    "line=0\n",
    "for lines in calibF:\n",
    "    if(line==1):\n",
    "        k= np.array(lines.strip('\\n').split(' '))\n",
    "    if(line==4):\n",
    "        bDash=lines.split(' ')\n",
    "        b=float(bDash[0])\n",
    "    line+=1\n",
    "kDash=k.astype('float64').reshape(3,3)\n",
    "\n",
    "worldPoints=[]\n",
    "pose_sh = []\n",
    "transformationList=np.array(read_transformations()).astype('float64')\n",
    "pose0 = np.vstack([transformationList[0], np.array([0, 0, 0, 1])])\n",
    "pose_sh.append(pose0)\n",
    "for i in range(1, len(transformationList)):\n",
    "    posey = np.vstack([transformationList[i], np.array([0, 0, 0, 1])])\n",
    "    pose_sh.append(posey)\n",
    "\n",
    "\n",
    "# calcuation of Q matrix\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "pathL='img2/'\n",
    "pathR='img3/'\n",
    "\n",
    "fl=7.070912e+02\n",
    "bl = 0.53790448812\n",
    "\n",
    "xc=185 # Centre pixel\n",
    "yc=613 # Centre pixel \n",
    "\n",
    "# calculation of Q m\n",
    "f=k[0][0]\n",
    "\n",
    "\n",
    "\n",
    "# imgLrGB = cv2.cvtColor('/img2/0000000460.png', cv2.COLOR_BGR2RGB)\n",
    "# grayRrGB = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "camPoints=[]\n",
    "colours=np.zeros((370, 1226, 3))\n",
    "thdoutputs=[]\n",
    "thdcolours=[]\n",
    "imgPointMap = {}\n",
    "imgPointMap_alt = {}\n",
    "imgPointMap_2 = {}\n",
    "\n",
    "NumOfPairs=20\n",
    "\n",
    "for i in range(NumOfPairs):\n",
    "    img2=pathL+'0'*7+str(460+i)+str('.png')\n",
    "    img3=pathR+'0'*7+str(460+i)+str('.png')\n",
    "    \n",
    "\n",
    "    imgLF=cv2.imread(img2)\n",
    "    imgRF=cv2.imread(img3)\n",
    "    \n",
    "    imgL=cv2.cvtColor(imgLF,cv2.COLOR_BGR2RGB)\n",
    "    imgR=cv2.cvtColor(imgRF,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    scale_percent = 100 # percent of original size\n",
    "    width = int(imgL.shape[1] * scale_percent / 100)\n",
    "    height = int(imgR.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height) \n",
    "    h, w = imgL.shape[:2]\n",
    "    print(h)\n",
    "    numOfPix= width*height\n",
    "    Q=np.array([[1,0,0,-w/2],[0,-1,0,h/2],[0,0,0,fl],[0,0,-bl**(-1),0]])\n",
    "    Q=Q.astype('float64')\n",
    "    \n",
    "#     imgL=cv2.resize(imgLF, dim, interpolation = cv2.INTER_jnreconAREA) \n",
    "#     imgR=cv2.resize(imgRF, dim, interpolation = cv2.INTER_AREA) \n",
    "\n",
    "    window_size = 3 \n",
    "    \n",
    "    minDisp = 16\n",
    "    numDisp = 112 - minDisp\n",
    "    \n",
    "    ImageDisp = cv2.StereoSGBM_create(\n",
    "        minDisparity=minDisp,             \n",
    "        blockSize=7,\n",
    "        numDisparities=numDisp + (2) * 16,\n",
    "        P1=8*3*window_size**2,    \n",
    "        P2=32*3*window_size**2,\n",
    "        uniquenessRatio=12,\n",
    "        speckleWindowSize=400,\n",
    "        disp12MaxDiff=1,\n",
    "        speckleRange=5,\n",
    "        #preFilterCap=63\n",
    "    )\n",
    "    \n",
    "    \n",
    "    dispMat = ImageDisp.compute(imgL, imgR).astype(np.float32)\n",
    "    print(dispMat)\n",
    "    \n",
    "   \n",
    "   \n",
    "    \n",
    "           \n",
    "    # cv2 way of finding 3d points\n",
    "    camPoints = cv2.reprojectImageTo3D(dispMat, Q)\n",
    "    colours = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    \n",
    "    imgPointMap[i] = []\n",
    "    imgPointMap_alt[i] = []\n",
    "    imgPointMap_2[i] = []\n",
    "    for k in range(len(imgL)):\n",
    "        for j in range(len(imgL[k])):\n",
    "            colours[k][j] = imgL[k][j]\n",
    "            pt_temp = Q @ np.array([k, j, dispMat[k][j]/16.0, 1]).T\n",
    "            W = pt_temp[3]\n",
    "            to_ap = [pt_temp[0]/W, pt_temp[1]/W, pt_temp[2]/W]\n",
    "            camPoints[k][j] = to_ap\n",
    "            imgPointMap[i].append(([k, j], to_ap))\n",
    "            if dispMat[i][j] > dispMat.min():\n",
    "                imgPointMap_alt[i].append(([k, j], to_ap))\n",
    "                # This step takes the above project 3d, and transforms them according to the ground truth poses.\n",
    "                pt_new_temp = np.array(newpoint(to_ap, pose_sh[i]).T).astype('float32')[0]\n",
    "                pt_add = [pt_new_temp[0], pt_new_temp[1], pt_new_temp[2]]\n",
    "                imgPointMap_2[i].append(([k, j], pt_add))\n",
    "\n",
    "    mask_map = dispMat > dispMat.min()\n",
    "    o_camPoints = camPoints[mask_map]\n",
    "    o_colours = colours[mask_map]\n",
    "    o_colours =(o_colours/255).astype('float64')\n",
    "    for j,pts in enumerate(o_camPoints):\n",
    "        thdoutputs.append(np.array(newpoint(pts, pose_sh[i]).T).astype('float32')[0])\n",
    "        thdcolours.append(o_colours[j])\n",
    "        \n",
    "thdoutputs = np.array(thdoutputs)\n",
    "thdcolours = np.array(thdcolours)\n",
    "\n",
    "#print(thdoutputs)\n",
    "# print(thdcolours.shape)\n",
    "\n",
    "accPntCld=[]\n",
    "pntCld= o3d.geometry.PointCloud()\n",
    "pntCld.points= o3d.utility.Vector3dVector(thdoutputs)\n",
    "pntCld.colors= o3d.utility.Vector3dVector(thdcolours)\n",
    "o3d.io.write_point_cloud(\"all_pcs.ply\", pntCld)\n",
    "o3d.visualization.draw_geometries([pntCld])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=colours\n",
    "# o3d.visualization.draw_geometries([pntCld])\n",
    "# img=np.array(cv2.imread('img2/'+'0'*7+str(460+0)+str('.png')))\n",
    "# print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############-------Obtaining world points............######\n",
    "\n",
    "\n",
    "# pt_new_temp = np.array(make_new_pt(to_ap, combination[count+start]).T).astype('float32')[0]\n",
    "# for i in range(NumOfPairs):\n",
    "#      worldPoints.append(transformationList[i].dot(camPoints[i]))\n",
    "\n",
    "# # worldPoints=np.array(worldPoints)\n",
    "\n",
    "# # siz=worldPoints.shape\n",
    "# # worldPoints=worldPoints.reshape(siz[1],siz[0]*siz[2])\n",
    "\n",
    "\n",
    "# # colours=temp\n",
    "# # colours=colours.reshape(siz[0]*siz[2],siz[1])\n",
    "# # print(colours.shape)\n",
    "\n",
    "# # # temp=np.ones((1,worldPoints.shape[2]))\n",
    "\n",
    "# # # np.row_stack((worldPoints[0],\n",
    "# # #           temp))\n",
    "\n",
    "# # # print(worldPoints[0].shape)\n",
    " \n",
    "# # # # for i in range(worldPoints.shape[0]):\n",
    "# # # #     np.concatenate((worldPoints[i],\n",
    "# # # #                np.ones((406260))),axis=1)\n",
    "\n",
    "\n",
    "# # # mmk commented below\n",
    "\n",
    "# # accPntCld=[]\n",
    "# # pntCld= o3d.geometry.PointCloud()\n",
    "# # pntCld.points= o3d.utility.Vector3dVector(worldPoints.T)\n",
    "# # pntCld.colors= o3d.utility.Vector3dVector(colours)\n",
    "# # o3d.io.write_point_cloud(\"all_pcs.ply\", pntCld)\n",
    "# # o3d.visualization.draw_geometries([pntCld])\n",
    "\n",
    "# #mmk commented\n",
    "\n",
    "\n",
    "# #     for x in range(height):\n",
    "# #         for y in range(width):\n",
    "# #             XL=(b/disparity[x-xc])*imgL[i,:2]\n",
    "# #             XL.append()\n",
    "# #             XR=(b/disparity[y-yc])*imgR[i,:2]\n",
    "# #     camPoints.append(XL)\n",
    "# #     camPoints.append(XR)\n",
    "    \n",
    "    \n",
    "# #   tempzero[:,0]=disparity\n",
    "    \n",
    "# #     disparity=tempz\n",
    "    \n",
    "# #     wrld= b*\n",
    "    \n",
    "# # print(dispMat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### PART 2: Motion estimation using iterative PnP\n",
    "\n",
    "Using the generated reconstruction from the previous part, synthesize a new image taken by a virtualmonocular camera fixed at any arbitrary position and orientation.  Your task in this part is to recoverthis pose using an iterative Perspective-from-n-Points (PnP) algorithm. \n",
    "\n",
    "#### Procedure: \n",
    "\n",
    "<ol>\n",
    "    <li> Obtain a set of 2D-3D correspondences between the the image and the point cloud.  Since hereyou’re generating the image, this should be easy to obtain. </li>\n",
    "    <li> For this set of correspondences compute the total reprojection error c= $\\sum_{i} ‖x_i−P_{k}X_i‖^2 $    where $P_{k}= K[R_{k}|t_{k}]$, $X_{i}$ is the 3D point in the world frame, $x_{i}$ is its corresponding projection. </li>\n",
    "    <li> Solve for the pose $T_{k}$ that minimizes this non-linear reprojection error using a Gauss-Newton (GN)scheme.  Recall that in GN we start with some initial estimated value $x_{o}$ and iteratively refine the estimate using $x_{1}$= $∆x+x_0$, where $∆x$ is obtained by solving the normal equations $J^{T}J∆x$= -$J^{T}e$, until convergence.The main steps in this scheme are computing the corresponding Jacobians and updating the estimates correctly.  For our problem,  use a 12×1 vector parameterization for $T_{k}$(the top 3×4submatrix).  Run the optimization for different choices of initialization and report your observations. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
